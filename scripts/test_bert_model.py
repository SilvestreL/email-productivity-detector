#!/usr/bin/env python3
"""
Script de teste para o modelo BERT fine-tuned para classificaÃ§Ã£o de emails
"""

import os
import sys
import torch
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TextClassificationPipeline,
)
import json
from typing import Dict, List
import time

# Adicionar o diretÃ³rio raiz ao path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


def load_model(model_path: str):
    """
    Carrega o modelo BERT fine-tuned

    Args:
        model_path: Caminho para o diretÃ³rio do modelo

    Returns:
        Tuple com (tokenizer, model, pipeline)
    """
    try:
        print(f"ğŸ”„ Carregando modelo de: {model_path}")

        # Verificar se o diretÃ³rio existe
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"DiretÃ³rio do modelo nÃ£o encontrado: {model_path}")

        # Carregar tokenizer
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        print("âœ… Tokenizer carregado com sucesso")

        # Carregar modelo
        model = AutoModelForSequenceClassification.from_pretrained(model_path)
        print("âœ… Modelo carregado com sucesso")

        # Configurar dispositivo
        device = 0 if torch.cuda.is_available() else -1
        device_name = "CUDA" if device == 0 else "CPU"
        print(f"ğŸ–¥ï¸  Dispositivo: {device_name}")

        # Criar pipeline
        pipeline = TextClassificationPipeline(
            model=model, tokenizer=tokenizer, return_all_scores=True, device=device
        )
        print("âœ… Pipeline criado com sucesso")

        return tokenizer, model, pipeline

    except Exception as e:
        print(f"âŒ Erro ao carregar modelo: {e}")
        return None, None, None


def test_classification(pipeline, test_texts: List[str]) -> List[Dict]:
    """
    Testa a classificaÃ§Ã£o com textos de exemplo

    Args:
        pipeline: Pipeline de classificaÃ§Ã£o
        test_texts: Lista de textos para testar

    Returns:
        Lista de resultados de classificaÃ§Ã£o
    """
    results = []

    for i, text in enumerate(test_texts):
        print(f"\nğŸ“§ Testando texto {i+1}:")
        print(f"Texto: {text[:100]}{'...' if len(text) > 100 else ''}")

        start_time = time.time()

        try:
            # Classificar texto
            result = pipeline(text)

            # Processar resultado
            scores = {}
            for pred in result[0]:
                # O modelo retorna labels como strings ("Improdutivo", "Produtivo")
                label_name = pred["label"]
                scores[label_name] = float(pred["score"])

            # Determinar categoria
            category = max(scores, key=scores.get)
            confidence = scores[category]

            inference_time = (time.time() - start_time) * 1000  # ms

            result_dict = {
                "text": text,
                "category": category,
                "confidence": confidence,
                "scores": scores,
                "inference_time_ms": round(inference_time, 2),
            }

            results.append(result_dict)

            print(f"âœ… Categoria: {category}")
            print(f"ğŸ“Š ConfianÃ§a: {confidence:.2%}")
            print(f"â±ï¸  Tempo: {inference_time:.0f}ms")
            print(f"ğŸ“ˆ Scores: {scores}")

        except Exception as e:
            print(f"âŒ Erro na classificaÃ§Ã£o: {e}")
            results.append({"text": text, "error": str(e)})

    return results


def main():
    """FunÃ§Ã£o principal"""
    print("ğŸš€ Teste do Modelo BERT para ClassificaÃ§Ã£o de Emails")
    print("=" * 60)

    # Caminho para o modelo
    model_path = "../models/bert_prod_improd"

    # Carregar modelo
    tokenizer, model, pipeline = load_model(model_path)

    if pipeline is None:
        print("âŒ Falha ao carregar modelo. Encerrando.")
        return

    print("\n" + "=" * 60)
    print("ğŸ§ª TESTES DE CLASSIFICAÃ‡ÃƒO")
    print("=" * 60)

    # Textos de teste
    test_texts = [
        # Email produtivo - solicita aÃ§Ã£o
        "Prezados, gostaria de solicitar uma reuniÃ£o para discutir o projeto de implementaÃ§Ã£o do novo sistema. Precisamos definir cronograma, recursos necessÃ¡rios e responsabilidades. Podem me informar a disponibilidade da equipe para prÃ³xima semana?",
        # Email produtivo - requer resposta
        "OlÃ¡, preciso de um orÃ§amento para desenvolvimento de aplicaÃ§Ã£o web. O projeto inclui sistema de login, dashboard administrativo e relatÃ³rios. Qual seria o prazo e valor estimado?",
        # Email improdutivo - apenas informaÃ§Ã£o
        "Bom dia a todos! Apenas para informar que hoje Ã© aniversÃ¡rio da empresa. Desejamos a todos um excelente dia!",
        # Email improdutivo - agradecimento
        "Obrigado pelo envio dos documentos. Recebemos tudo em ordem.",
        # Email produtivo - problema tÃ©cnico
        "Estamos enfrentando problemas com o servidor de produÃ§Ã£o. O sistema estÃ¡ apresentando lentidÃ£o e alguns usuÃ¡rios nÃ£o conseguem acessar. Precisamos de suporte urgente para resolver esta situaÃ§Ã£o.",
        # Email improdutivo - piada/descontraÃ­do
        "Por que o programador foi ao mÃ©dico? Porque estava com bugs! ğŸ˜„ Bom dia a todos!",
    ]

    # Executar testes
    results = test_classification(pipeline, test_texts)

    print("\n" + "=" * 60)
    print("ğŸ“Š RESUMO DOS TESTES")
    print("=" * 60)

    # EstatÃ­sticas
    successful_tests = [r for r in results if "error" not in r]
    failed_tests = [r for r in results if "error" in r]

    print(f"âœ… Testes bem-sucedidos: {len(successful_tests)}")
    print(f"âŒ Testes com erro: {len(failed_tests)}")

    if successful_tests:
        avg_confidence = sum(r["confidence"] for r in successful_tests) / len(
            successful_tests
        )
        avg_time = sum(r["inference_time_ms"] for r in successful_tests) / len(
            successful_tests
        )

        print(f"ğŸ“Š ConfianÃ§a mÃ©dia: {avg_confidence:.2%}")
        print(f"â±ï¸  Tempo mÃ©dio: {avg_time:.0f}ms")

        # Contar categorias
        categories = {}
        for r in successful_tests:
            cat = r["category"]
            categories[cat] = categories.get(cat, 0) + 1

        print(f"ğŸ·ï¸  DistribuiÃ§Ã£o de categorias: {categories}")

    # Salvar resultados
    output_file = "test_results.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ’¾ Resultados salvos em: {output_file}")

    print("\n" + "=" * 60)
    print("ğŸ¯ PRÃ“XIMOS PASSOS")
    print("=" * 60)
    print("1. âœ… Modelo testado com sucesso")
    print("2. ğŸ”„ Integrar no Streamlit app")
    print("3. ğŸš€ Publicar no Hugging Face Hub")
    print("4. ğŸ“Š Monitorar performance em produÃ§Ã£o")


if __name__ == "__main__":
    main()
